{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNLJzCyZqY03LdgAJHsxU2K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dhanushranga1/AIW4-MambaModels/blob/main/mamba_with_zeta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U zetascale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9sU0VTQr-i1h",
        "outputId": "0a5eb24d-a69b-4139-c713-b6142c4ea19b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting zetascale\n",
            "  Downloading zetascale-2.8.6-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from zetascale) (1.7.0)\n",
            "Collecting argparse<2.0.0,>=1.4.0 (from zetascale)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting beartype (from zetascale)\n",
            "  Downloading beartype-0.21.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting bitsandbytes (from zetascale)\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Collecting colt5-attention (from zetascale)\n",
            "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from zetascale) (0.8.1)\n",
            "Collecting einops-exts==0.0.4 (from zetascale)\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Collecting joblib<1.4.0,>=1.3.0 (from zetascale)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting local-attention (from zetascale)\n",
            "  Downloading local_attention-1.11.1-py3-none-any.whl.metadata (907 bytes)\n",
            "Collecting loguru (from zetascale)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from zetascale) (13.9.4)\n",
            "Collecting scikit-learn<1.6.0,>=1.5.0 (from zetascale)\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from zetascale) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from zetascale) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from zetascale) (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from zetascale) (4.52.4)\n",
            "Collecting vector-quantize-pytorch (from zetascale)\n",
            "  Downloading vector_quantize_pytorch-1.22.17-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.5.0->zetascale) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (0.33.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.20.0->zetascale) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->zetascale) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->zetascale)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->zetascale)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->zetascale)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->zetascale)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->zetascale)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->zetascale)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->zetascale)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->zetascale)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->zetascale)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->zetascale)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->zetascale) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->zetascale) (1.3.0)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention->zetascale)\n",
            "  Downloading hyper_connections-0.2.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->zetascale) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->zetascale) (2.19.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->zetascale) (11.2.1)\n",
            "Collecting einx>=0.3.0 (from vector-quantize-pytorch->zetascale)\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->vector-quantize-pytorch->zetascale) (2.4.6)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers<5.0.0,>=4.20.0->zetascale) (1.1.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->zetascale) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->zetascale) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers<5.0.0,>=4.20.0->zetascale) (2025.6.15)\n",
            "Downloading zetascale-2.8.6-py3-none-any.whl (533 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.8/533.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m123.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.21.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Downloading local_attention-1.11.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vector_quantize_pytorch-1.22.17-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hyper_connections-0.2.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: argparse, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, loguru, joblib, einops-exts, beartype, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, einx, nvidia-cusolver-cu12, vector-quantize-pytorch, hyper-connections, bitsandbytes, local-attention, colt5-attention, zetascale\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.1\n",
            "    Uninstalling joblib-1.5.1:\n",
            "      Successfully uninstalled joblib-1.5.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed argparse-1.4.0 beartype-0.21.0 bitsandbytes-0.46.0 colt5-attention-0.11.1 einops-exts-0.0.4 einx-0.3.0 hyper-connections-0.2.1 joblib-1.3.2 local-attention-1.11.1 loguru-0.7.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 scikit-learn-1.5.2 vector-quantize-pytorch-1.22.17 zetascale-2.8.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "c24e27d3cd3c49a1a2f685e644d4c477"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NXN25oSb9h-h"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from zeta import SSM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "\n",
        "# Dummy SSM for testing\n",
        "class SSM(nn.Module):\n",
        "    def __init__(self, dim, dt_rank, dim_inner, d_state):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "class CobraBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        dt_rank: int,\n",
        "        dim_inner: int,\n",
        "        d_state: int,\n",
        "        channels: int = 64\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(dim, channels)\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=channels,\n",
        "            out_channels=channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            dilation=1,\n",
        "            groups=1\n",
        "        )\n",
        "\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "        self.ssm = SSM(\n",
        "            channels,\n",
        "            dt_rank,\n",
        "            dim_inner,\n",
        "            d_state\n",
        "        )\n",
        "\n",
        "        self.output_proj = nn.Linear(channels, dim)\n",
        "\n",
        "    def forward(self, x: Tensor):\n",
        "        # x: [batch, seq_len, dim]\n",
        "        skip = x\n",
        "        x = self.input_proj(x)  # [B, L, C]\n",
        "\n",
        "        x_one = x.transpose(1, 2)          # [B, C, L] for conv1d\n",
        "        x_one = self.conv(x_one)\n",
        "        x_one = self.swish(x_one)\n",
        "        x_one = x_one.transpose(1, 2)      # back to [B, L, C]\n",
        "        x_one = self.ssm(x_one)\n",
        "\n",
        "        x_two = self.swish(x)\n",
        "\n",
        "        # Apply element-wise multiplication instead of matmul for same shape\n",
        "        out = x_one * x_two\n",
        "\n",
        "        out = out + x  # Residual connection (after proj)\n",
        "        out = self.output_proj(out)\n",
        "        out = out + skip  # Final residual\n",
        "\n",
        "        return out\n",
        "\n",
        "# # Example input: [batch, seq_len, dim]\n",
        "# x = torch.randn(1, 64, 256)\n",
        "# block = CobraBlock(\n",
        "#     dim=256,\n",
        "#     dt_rank=8,\n",
        "#     dim_inner=256,\n",
        "#     d_state=256\n",
        "# )\n",
        "# out = block(x)\n",
        "# print(out.shape)  # Should print: torch.Size([1, 64, 256])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72WueEaa-gcg",
        "outputId": "93388d78-8376-40a2-8591-da45fb65c1c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from zeta.nn.modules import TextTokenEmbedding  # Ensure this module is correctly installed\n",
        "\n",
        "# Assume CobraBlock is already defined and working as fixed earlier\n",
        "# from cobra_block import CobraBlock  # You must import your working CobraBlock here\n",
        "\n",
        "class Cobra(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        dt_rank: int,\n",
        "        dim_inner: int,\n",
        "        d_state: int,\n",
        "        channels: int = 64,\n",
        "        num_tokens: int = 10000,\n",
        "        depth: int = 12,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.dt_rank = dt_rank\n",
        "        self.dim_inner = dim_inner\n",
        "        self.d_state = d_state\n",
        "        self.channels = channels\n",
        "        self.num_tokens = num_tokens\n",
        "        self.depth = depth\n",
        "\n",
        "        # Token embeddings\n",
        "        self.embed = TextTokenEmbedding(\n",
        "            dim,\n",
        "            num_tokens,\n",
        "            l2norm_embed=True\n",
        "        )\n",
        "\n",
        "        # Transformer-style layers\n",
        "        self.layers = nn.ModuleList([\n",
        "            CobraBlock(\n",
        "                dim,\n",
        "                dt_rank,\n",
        "                dim_inner,\n",
        "                d_state,\n",
        "                channels,\n",
        "                *args,\n",
        "                **kwargs\n",
        "            ) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Final normalization\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x: [batch, sequence] -> Token indices\n",
        "        x = self.embed(x)  # Output: [batch, seq_len, dim]\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "# Test input\n",
        "x = torch.randint(0, 10000, (1, 64))  # [batch, seq_len]\n",
        "model = Cobra(\n",
        "    dim=256,\n",
        "    dt_rank=8,\n",
        "    dim_inner=256,\n",
        "    d_state=256,\n",
        "    channels=64,\n",
        "    num_tokens=10000,\n",
        "    depth=12\n",
        ")\n",
        "out = model(x)\n",
        "print(out.shape)  # Should be torch.Size([1, 64, 256])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucMeqaxfETuo",
        "outputId": "148e9f78-660c-494c-e346-ec083601fdc5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 64, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randint(0, model.num_tokens, (2, 64))  # batch=2, seq_len=64\n",
        "out = model(x)  # Should output: [2, 64, 256]\n",
        "print(\"Output shape:\", out.shape)\n",
        "\n",
        "# Gradient check\n",
        "out.sum().backward()\n",
        "print(\"Gradient check passed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GVakItIIOQk",
        "outputId": "7fd24bab-f9c9-4435-a8d4-e48e305d9b2d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 64, 256])\n",
            "Gradient check passed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "# dummy input: batch = 2, seq_len = 64\n",
        "x = torch.randint(0, 10000, (2, 64))\n",
        "\n",
        "model = Cobra(\n",
        "    dim=256,\n",
        "    dt_rank=8,\n",
        "    dim_inner=256,\n",
        "    d_state=256,\n",
        "    channels=64,\n",
        "    num_tokens=10000,\n",
        "    depth=4\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    out = model(x)\n",
        "    end = time.time()\n",
        "\n",
        "print(out.shape)\n",
        "print(\"inference time:\", round((end - start) * 1000, 2), \"ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFY19k2CLVbo",
        "outputId": "2b5f7784-7434-48b5-d090-51ca195a7e26"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 64, 256])\n",
            "inference time: 3.69 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k93QdfL1MCpb",
        "outputId": "d36735fa-57b9-4915-9d55-989f30f7fc48"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# input_size: (batch_size, seq_len)\n",
        "summary(model, input_size=(2, 64), device=\"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJZnxvSBLlbh",
        "outputId": "4164487a-676b-4708-df41-688f7fd21483"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Cobra                                    [2, 64, 256]              --\n",
              "├─TextTokenEmbedding: 1-1                [2, 64, 256]              --\n",
              "│    └─Embedding: 2-1                    [2, 64, 256]              2,560,000\n",
              "├─ModuleList: 1-2                        --                        --\n",
              "│    └─CobraBlock: 2-2                   [2, 64, 256]              --\n",
              "│    │    └─Linear: 3-1                  [2, 64, 64]               16,448\n",
              "│    │    └─Conv1d: 3-2                  [2, 64, 64]               12,352\n",
              "│    │    └─SiLU: 3-3                    [2, 64, 64]               --\n",
              "│    │    └─SSM: 3-4                     [2, 64, 64]               4,160\n",
              "│    │    └─SiLU: 3-5                    [2, 64, 64]               --\n",
              "│    │    └─Linear: 3-6                  [2, 64, 256]              16,640\n",
              "│    └─CobraBlock: 2-3                   [2, 64, 256]              --\n",
              "│    │    └─Linear: 3-7                  [2, 64, 64]               16,448\n",
              "│    │    └─Conv1d: 3-8                  [2, 64, 64]               12,352\n",
              "│    │    └─SiLU: 3-9                    [2, 64, 64]               --\n",
              "│    │    └─SSM: 3-10                    [2, 64, 64]               4,160\n",
              "│    │    └─SiLU: 3-11                   [2, 64, 64]               --\n",
              "│    │    └─Linear: 3-12                 [2, 64, 256]              16,640\n",
              "│    └─CobraBlock: 2-4                   [2, 64, 256]              --\n",
              "│    │    └─Linear: 3-13                 [2, 64, 64]               16,448\n",
              "│    │    └─Conv1d: 3-14                 [2, 64, 64]               12,352\n",
              "│    │    └─SiLU: 3-15                   [2, 64, 64]               --\n",
              "│    │    └─SSM: 3-16                    [2, 64, 64]               4,160\n",
              "│    │    └─SiLU: 3-17                   [2, 64, 64]               --\n",
              "│    │    └─Linear: 3-18                 [2, 64, 256]              16,640\n",
              "│    └─CobraBlock: 2-5                   [2, 64, 256]              --\n",
              "│    │    └─Linear: 3-19                 [2, 64, 64]               16,448\n",
              "│    │    └─Conv1d: 3-20                 [2, 64, 64]               12,352\n",
              "│    │    └─SiLU: 3-21                   [2, 64, 64]               --\n",
              "│    │    └─SSM: 3-22                    [2, 64, 64]               4,160\n",
              "│    │    └─SiLU: 3-23                   [2, 64, 64]               --\n",
              "│    │    └─Linear: 3-24                 [2, 64, 256]              16,640\n",
              "├─LayerNorm: 1-3                         [2, 64, 256]              512\n",
              "==========================================================================================\n",
              "Total params: 2,758,912\n",
              "Trainable params: 2,758,912\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.MEGABYTES): 11.74\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 2.36\n",
              "Params size (MB): 11.04\n",
              "Estimated Total Size (MB): 13.40\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PygnBtmQMBtq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}